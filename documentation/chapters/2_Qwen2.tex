\chapter{Large Language Model (LLM)}

This chapter outlines the technologies and tools employed in the development of the project. These include the Large Language Models (LLMs) Qwen2.5 and LLaMA. By utilizing these technologies, a robust and efficient system was created for processing natural language data and generating text outputs.

A Large Language Model (LLM) is a computational system designed to generate language and perform various natural language processing (NLP) tasks. These models learn statistical patterns and relationships from vast amounts of text during a self-supervised or semi-supervised training process. As a result, LLMs excel in tasks such as text generation, translation, summarization, and more.

\section{Qwen2.5}

Qwen2.5 is an open-source large language model developed by Alibaba group. Trained on a vast corpus of text, it has been fine-tuned for applications such as text classification, named entity recognition, and sentiment analysis. Qwen2.5 is designed for efficiency and scalability, allowing it to process large datasets in real-time across multiple hardware platforms, including CPUs, GPUs, and TPUs.

This model achieves high precision and recall, delivering accurate outcomes for a variety of NLP tasks. Its adaptability enables seamless integration across different domains and languages. Security is a key feature of Qwen2.5, making it suitable for processing sensitive information, such as personal, financial, or medical data, without compromising privacy.

As an open-source model, Qwen2.5 allows the community to access, modify, and enhance its architecture, fostering innovation and transparency. The model operates locally, ensuring that data remains on the user's device, thus preventing data from being transmitted to external servers or third parties. By retaining control over their data, users can ensure privacy and security standards are upheld.

The \textbf{Qwen2.5 1.5B model} was selected for this project, providing the necessary capabilities for various tasks. A Python application was developed to run the model locally, leveraging the power of the \textit{NVIDIA 3070 GPU}. This setup ensures efficient processing without requiring external servers, offering full control over the data while maintaining privacy standards.

\section{LLaMA}

LLaMA (Large Language Model Meta AI) is a series of advanced open-source language models developed by Meta (formerly Facebook), designed to provide powerful language processing capabilities with a smaller resource footprint compared to other large models. LLaMA was released as an open-source project to promote collaborative research and experimentation within the AI community. The models vary in size, from smaller versions with billions of parameters to larger models with tens of billions of parameters, allowing users to choose a model that balances performance with computational efficiency.

LLaMA prioritizes efficiency, making it more accessible to individuals and organizations without the extensive computing resources typically needed for larger models. This design enables LLaMA to be used in a variety of applications, including text generation, summarization, translation, and question-answering. Meta's decision to open-source LLaMA aims to foster transparency, collaboration, and innovation, encouraging the community to explore, fine-tune, and customize the model for specific use cases. This initiative supports both academic and commercial research, positioning LLaMA as a versatile tool for natural language processing.

In this project, LLaMA was used to compare results obtained from Qwen2.5, offering insights into the performance and capabilities of different large language models. By leveraging LLaMA's features, a comprehensive evaluation was conducted, examining the models' performance across various NLP tasks and highlighting the similarities and differences in their language processing capabilities.